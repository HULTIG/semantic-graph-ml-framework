{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# SAFIR Workflow Demonstration Notebook\n",
   "id": "13d9cd89612d2e79"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [RDF Semantic Evaluation](#RDF-Semantic-Evaluation)\n",
    "3. [Graph Construction](#Graph-Construction)\n",
    "4. [Feature Engineering](#Feature-Engineering)\n",
    "5. [Model Training](#Model-Training)\n",
    "6. [Adaptation Triggering](#Adaptation-Triggering)\n",
    "7. [Evaluation](#Evaluation)\n",
    "8. [Continuous Learning](#Continuous-Learning) "
   ],
   "id": "f5dcf763eadb4055"
  },
  {
   "cell_type": "markdown",
   "id": "f39baf9291986a77",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates a sample use case to process input data from healthcare and environmental readings of a user. \n",
    "\n",
    "The workflow includes the implementation of graph learning into the process using the following steps: \n",
    "1. **Load input data**: JSON data representing devices, sensors, actuators, and events;\n",
    "2. **Transform the data into RDF format using an external API**: the api service is running in the same network as the notebook and can be accessed at `http://api:8080/api/v1/rdf/gi/generate-rdf`, the data is transformed into RDF format using the R2RML transformation API;\n",
    "3. **Graph construction**: the RDF data is converted into a NetworkX graph for further analysis and visualization;\n",
    "4. **Feature engineering**: by extracting features from the graph data;\n",
    "5. **Model training**: a sample Graph Neural Network (GNN) is trained to classify patient risk levels based on graph structure and features;\n",
    "6. **Adaptation triggering**: a simulation of notifications or alerts are triggered for high-risk patients;\n",
    "7. **Evaluation**: the performance and utility of the generated RDF graph are evaluated through visualizations and metrics.\n",
    "8. **Visualization**: the graph is visualized using NetworkX.\n",
    "9. **Metrics Calculation**: key metrics are calculated for the workflow.\n",
    "10. **Continuous Learning**: the graph learning process is repeated for new data.\n",
    "\n",
    "### Use Case Demonstration in Self-Adaptive Systems\n",
    "\n",
    "#### Node Classification for Patient Monitoring\n",
    "\n",
    "- **Scenario**: Predict the risk level of a patient (e.g. low, medium, high).\n",
    "- **Graph Input**:\n",
    "  - **Nodes**: Patients, observations, devices.\n",
    "  - **Features**: Sensor readings (e.g. temperature), patient profiles.\n",
    "- **Learning Task**: Train a Graph Neural Network (GNN) to classify patient risk levels based on graph structure and features.\n",
    "- **Adaptation**: Trigger notifications or alerts for high-risk patients.\n",
    "\n",
    "This use case demonstrates how graph learning can be applied to self-adaptive systems, enabling real-time monitoring and automated decision-making in critical applications such as patient care.\n",
    "\n",
    "In summary, the workflow consists of the following steps:\n",
    "1. **Achieving Semantic Data Interoperability**: Transforming JSON input data into RDF, making it machine-readable and semantically enriched.\n",
    "2. **Graph Learning and Visualization**: Constructing and analyzing the RDF graph, with metrics calculation for insights.\n",
    "3. **Metrics Calculation**: Evaluating the performance and utility of the generated RDF graph through visualizations and metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647fea21-afd2-483c-8852-0c881add00c7",
   "metadata": {},
   "source": [
    "Uncomment the following line if you haven't installed the necessary dependencies yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c32bcb6881f9ebf",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Install the required packages\n",
    "# !pip install -r requirements.txt"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6b683cea-da83-435e-baaf-4d41a4468c68",
   "metadata": {},
   "source": [
    "Dependencies may include libraries for RDF processing, machine learning, and graph visualization."
   ]
  },
  {
   "cell_type": "code",
   "id": "654e8c6496a946af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T16:35:21.375932Z",
     "start_time": "2024-12-13T16:35:04.759398Z"
    }
   },
   "source": [
    "# Importing required libraries\n",
    "import requests  # For making HTTP requests to external APIs\n",
    "import rdflib  # For working with RDF data and generating RDF graphs\n",
    "import networkx as nx  # For graph-based learning and visualization\n",
    "import matplotlib.pyplot as plt  # For creating visualizations such as plots and graphs\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np  # For numerical operations and handling arrays\n",
    "from sklearn.metrics import classification_report, roc_auc_score  # For evaluating model performance\n",
    "import os  # For interacting with the file system"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd2ce626cd63f2f4",
   "metadata": {},
   "source": [
    "# Constants and Configuration\n",
    "\n",
    "# The base URL for the API that generates RDF data\n",
    "API_URL = \"http://api:8080/api/v1/rdf/gi/generate-rdf\"\n",
    "\n",
    "# File paths for storing the RDF output and the generated graph\n",
    "RDF_OUTPUT_FILE = \"output.ttl\"\n",
    "GRAPH_OUTPUT_FILE = \"graph.gpickle\""
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dc9eee35-4217-4b8e-805f-eb8b69642a09",
   "metadata": {},
   "source": [
    "These paths are used to save the results of the RDF transformation and graph learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b8577d-613a-446b-9208-97f53e2589e8",
   "metadata": {},
   "source": [
    "The JSON contains sample data for various devices, sensors, actuators, and events.\n",
    "\n",
    "This data represents interactions between these entities and will be processed to generate RDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a7d6ccc76c69895",
   "metadata": {},
   "source": [
    "# Define the JSON input\n",
    "json_input = {\n",
    "    \"dataModel\": \"source_a\",  # Data model identifier\n",
    "    \"jsonData\": {\n",
    "        \"devices\": [\n",
    "            {\n",
    "                \"id\": \"dev_001\",  # Device identifier\n",
    "                \"name\": \"Primary Device\",  # Device name\n",
    "                \"serialNumber\": \"SN123456\",  # Serial number of the device\n",
    "                \"owner\": \"usr_001\"  # Owner identifier of the device\n",
    "            }\n",
    "        ],\n",
    "        \"actuators\": [\n",
    "            {\n",
    "                \"id\": \"act_001\",  # Actuator identifier\n",
    "                \"deviceId\": \"dev_001\"  # Associated device identifier\n",
    "            }\n",
    "        ],\n",
    "        \"uiDevices\": [\n",
    "            {\n",
    "                \"id\": \"ui_001\",  # UI Device identifier\n",
    "                \"sensorId\": \"sens_001\",  # Associated sensor identifier\n",
    "                \"actuatorId\": \"act_001\",  # Associated actuator identifier\n",
    "                \"deviceId\": \"dev_001\"  # Associated device identifier\n",
    "            }\n",
    "        ],\n",
    "        \"sensors\": [\n",
    "            {\n",
    "                \"id\": \"sens_001\",  # Sensor identifier\n",
    "                \"type\": \"TEMPERATURE\",  # Sensor type\n",
    "                \"location\": \"Living Room\",  # Sensor location\n",
    "                \"threshold\": 35.0,  # Threshold for sensor values\n",
    "                \"deviceId\": \"dev_001\"  # Associated device identifier\n",
    "            }\n",
    "        ],\n",
    "        \"events\": [\n",
    "            {\n",
    "                \"timestamp\": \"2021-09-29T11:19:11.788Z\",  # Event timestamp\n",
    "                \"id\": \"evt_001\",  # Event identifier\n",
    "                \"userId\": \"usr_002\",  # User associated with the event\n",
    "                \"assetId\": \"asset_001\",  # Associated asset\n",
    "                \"dimension\": \"VALUE\",  # Event dimension (e.g., temperature)\n",
    "                \"value\": 28.7,  # Event value (e.g., sensor reading)\n",
    "                \"originId\": \"sens_001\",  # Originating sensor\n",
    "                \"originType\": \"TEMPERATURE\"  # Type of originating sensor\n",
    "            },\n",
    "            {\n",
    "                \"timestamp\": \"2024-06-01T00:00:00.000Z\",  # Event timestamp\n",
    "                \"id\": \"evt_002\",  # Event identifier\n",
    "                \"userId\": \"usr_003\",  # User associated with the event\n",
    "                \"assetId\": \"asset_002\",  # Associated asset\n",
    "                \"dimension\": \"VALUE\",  # Event dimension (e.g., humidity)\n",
    "                \"value\": 38.24,  # Event value (e.g., sensor reading)\n",
    "                \"originId\": \"sens_002\",  # Originating sensor\n",
    "                \"originType\": \"HUMIDITY\"  # Type of originating sensor\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68e014d74d5c285",
   "metadata": {},
   "source": [
    "# Call the external API for RDF generation\n",
    "def generate_rdf(json_data):\n",
    "    \"\"\"\n",
    "    Send JSON input to the external RML API and receive RDF in turtle format.\n",
    "    \"\"\"\n",
    "    response = requests.post(API_URL, json=json_data)\n",
    "    if response.status_code == 200:\n",
    "        # Save RDF to file\n",
    "        with open(RDF_OUTPUT_FILE, \"w\") as rdf_file:\n",
    "            rdf_file.write(response.text)\n",
    "        print(\"RDF generation successful. Saved to:\", RDF_OUTPUT_FILE)\n",
    "    else:\n",
    "        print(\"RDF generation failed with status code:\", response.status_code)\n",
    "        response.raise_for_status()\n",
    "\n",
    "generate_rdf(json_input)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91bb72892dcddfcc",
   "metadata": {},
   "source": [
    "# Convert RDF to NetworkX graph\n",
    "def rdf_to_graph(rdf_graph):\n",
    "    \"\"\"\n",
    "    Convert RDF triples to a NetworkX graph.\n",
    "    \"\"\"\n",
    "    nx_graph = nx.DiGraph()\n",
    "    for subj, pred, obj in rdf_graph:\n",
    "        nx_graph.add_edge(str(subj), str(obj), predicate=str(pred))\n",
    "    print(\"Converted RDF to NetworkX graph with\", len(nx_graph.nodes), \"nodes and\", len(nx_graph.edges), \"edges.\")\n",
    "    return nx_graph\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "287638bf39e2ea66",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "with open(RDF_OUTPUT_FILE, \"r\") as rdf_file:\n",
    "    rdf_content = rdf_file.read()\n",
    "    print(rdf_content)"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## RDF Semantic Evaluation\n",
    "\n",
    "We will evaluate the generated RDF data for semantic completeness and vocabulary usage. We will compare the initial RML (RDF Mapping Language) transformation with the enriched RDF data.\n",
    "\n",
    "The initial RDF and the enriched RDF is available in the `input.ttl` and `output.ttl` files respectively, for comparison."
   ],
   "id": "c0892585c871105f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from rdflib import Graph, Literal\n",
    "from rdflib.namespace import XSD"
   ],
   "id": "ab379cb6c900e3e2",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def calculate_semantic_metrics(rdf_file):\n",
    "    \"\"\"\n",
    "    Calculate semantic metrics for RDF data with handling for invalid literals.\n",
    "    \"\"\"\n",
    "    g = Graph()\n",
    "    g.parse(rdf_file, format=\"turtle\")\n",
    "    total_triples = len(g)\n",
    "\n",
    "    # Define relevant vocabularies for semantic enrichment\n",
    "    vocabularies = [\n",
    "        \"http://www.w3.org/ns/sosa/\",\n",
    "        \"http://hl7.org/fhir/\",\n",
    "        \"http://schema.org/\",\n",
    "        \"http://unitsofmeasure.org/\"\n",
    "    ]\n",
    "\n",
    "    # Count triples that use semantic vocabularies\n",
    "    semantic_triples = 0\n",
    "    vocab_usage = {vocab: 0 for vocab in vocabularies}\n",
    "\n",
    "    for s, p, o in g:\n",
    "        try:\n",
    "            # Attempt to convert the literal value to its datatype\n",
    "            if isinstance(o, Literal) and o.datatype == XSD.dateTime:\n",
    "                # Validate date-time\n",
    "                o.toPython()\n",
    "        except Exception as e:\n",
    "            print(f\"Invalid value detected: {o} - Error: {e}\")\n",
    "\n",
    "        for vocab in vocabularies:\n",
    "            if vocab in str(p) or vocab in str(o):\n",
    "                semantic_triples += 1\n",
    "                vocab_usage[vocab] += 1\n",
    "\n",
    "    # Calculate semantic completeness\n",
    "    semantic_completeness = (semantic_triples / total_triples) * 100\n",
    "\n",
    "    # Display results\n",
    "    print(\"Total Triples:\", total_triples)\n",
    "    print(\"Semantic Triples:\", semantic_triples)\n",
    "    print(\"Semantic Completeness (%):\", round(semantic_completeness, 2))\n",
    "    print(\"\\nVocabulary Usage:\")\n",
    "    for vocab, count in vocab_usage.items():\n",
    "        print(f\"  {vocab}: {count} triples\")\n",
    "\n",
    "    return semantic_completeness, vocab_usage"
   ],
   "id": "a9516123c0a70122",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_rdf_file = \"input.ttl\"\n",
    "calculate_semantic_metrics(input_rdf_file)"
   ],
   "id": "c6063690681fca2e",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "output_rdf_file = \"output.ttl\"\n",
    "calculate_semantic_metrics(output_rdf_file)"
   ],
   "id": "952a14277769032b",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Graph Construction\n",
    "\n",
    "We will use `rdflib` to parse RDF data and construct a graph using `networkx`. The graph will represent entities and their relationships derived from the RDF triples.\n",
    "\n",
    "For example we have the following RDF data:"
   ],
   "id": "af0d26cc2d45b1d2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T20:41:03.887346Z",
     "start_time": "2024-12-13T20:41:03.593172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import rdflib\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example RDF data (you can replace this with your RDF content)\n",
    "sample_rdf_data = \"\"\"\n",
    "@prefix ex: <http://example.org/> .\n",
    "ex:dev_001 ex:hasDeviceName \"Primary Device\" .\n",
    "ex:dev_001 ex:hasOwner ex:usr_001 .\n",
    "ex:usr_001 ex:hasUserName \"User One\" .\n",
    "\"\"\"\n",
    "\n",
    "# Create a graph from RDF data\n",
    "sample_graph = rdflib.Graph()\n",
    "sample_graph.parse(data=sample_rdf_data, format=\"turtle\")\n",
    "\n",
    "# Convert RDF graph into a NetworkX graph\n",
    "sample_nx_graph = nx.Graph()\n",
    "\n",
    "# Add nodes and edges from RDF triples\n",
    "for subj, pred, obj in sample_graph:\n",
    "    # Convert RDF resources to strings\n",
    "    subj = str(subj)\n",
    "    obj = str(obj)\n",
    "\n",
    "    # Add nodes (subjects and objects)\n",
    "    sample_nx_graph.add_node(subj)\n",
    "    sample_nx_graph.add_node(obj)\n",
    "\n",
    "    # Add edge between subject and object (predicate is the relationship)\n",
    "    sample_nx_graph.add_edge(subj, obj, label=str(pred))\n",
    "\n",
    "# Visualize the graph\n",
    "plt.figure(figsize=(8, 8))\n",
    "nx.draw(sample_nx_graph, with_labels=True, node_size=2000, node_color='skyblue', font_size=10)\n",
    "plt.title(\"Graph constructed from RDF data\")\n",
    "plt.show()\n"
   ],
   "id": "536b0f6bb746f337",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import rdflib\n",
    "import networkx as nx\n",
    "import pickle\n",
    "\n",
    "# Parse RDF file\n",
    "rdf_graph = rdflib.Graph()\n",
    "rdf_graph.parse(RDF_OUTPUT_FILE, format=\"turtle\")\n",
    "\n",
    "# Convert RDF to NetworkX graph\n",
    "nx_graph = rdf_to_graph(rdf_graph)\n",
    "\n",
    "# Write NetworkX graph to pickle file\n",
    "with open(GRAPH_OUTPUT_FILE, 'wb') as f:\n",
    "    pickle.dump(nx_graph, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(f\"Converted RDF to NetworkX graph and saved to {GRAPH_OUTPUT_FILE}.\")\n"
   ],
   "id": "9be2f06c9e1b8cf4",
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c55d37e289512497",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T20:41:07.319575Z",
     "start_time": "2024-12-13T20:41:07.061019Z"
    }
   },
   "source": [
    "# Visualize the graph\n",
    "def visualize_graph(graph):\n",
    "    \"\"\"\n",
    "    Visualize the graph using NetworkX.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    pos = nx.spring_layout(graph)\n",
    "    nx.draw(graph, pos, with_labels=True, node_size=700, node_color=\"lightblue\", font_size=10, font_weight=\"bold\")\n",
    "    edge_labels = nx.get_edge_attributes(graph, \"predicate\")\n",
    "    nx.draw_networkx_edge_labels(graph, pos, edge_labels=edge_labels, font_size=8)\n",
    "    plt.title(\"Knowledge Graph Visualization\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_graph(nx_graph)"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "76d1c1dba8b0658b",
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T21:01:44.865947Z",
     "start_time": "2024-12-13T21:01:44.852438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_metrics(graph):\n",
    "    \"\"\"\n",
    "    Calculate and display key metrics for the RDF graph.\n",
    "    \"\"\"\n",
    "    # Graph structure metrics\n",
    "    num_nodes = len(graph.nodes)\n",
    "    num_edges = len(graph.edges)\n",
    "    avg_degree = sum(dict(graph.degree).values()) / num_nodes\n",
    "\n",
    "    # Report metrics\n",
    "    print(\"\\n--- Metrics ---\")\n",
    "    print(f\"Number of Nodes: {num_nodes}\")\n",
    "    print(f\"Number of Edges: {num_edges}\")\n",
    "    print(f\"Average Node Degree: {avg_degree:.2f}\")\n",
    "\n",
    "calculate_metrics(nx_graph)"
   ],
   "id": "7f1e252455e8cdec",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Features are extracted from RDF triples to represent nodes in a format suitable for graph neural networks (GNNs)."
   ],
   "id": "aa07f4c8993d957a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Uncomment the following line if you haven't installed the necessary dependencies yet.\n",
    "# !pip install torch\n",
    "# !pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.13.0+cu116.html"
   ],
   "id": "f49c6bf2f687388d",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for node in nx_graph.nodes:\n",
    "    print(type(node), node)\n",
    "\n",
    "node_to_index = {node.lower(): i for i, node in enumerate(nx_graph.nodes)}\n",
    "print(node_to_index)\n"
   ],
   "id": "c402acdb2ce64162",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Example: Create features for nodes\n",
    "node_features = {}\n",
    "for node in nx_graph.nodes:\n",
    "    # Simple feature extraction based on node label\n",
    "    if \"dev\" in node:\n",
    "        node_features[node] = np.random.rand(10)  # Random feature vector for device\n",
    "    else:\n",
    "        node_features[node] = np.zeros(10)  # Default feature for non-device nodes\n",
    "\n",
    "# Encode node labels to numeric indices using LabelEncoder\n",
    "node_labels = list(nx_graph.nodes)  # Extract all node labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(node_labels)\n",
    "\n",
    "# Create mapping from original node labels to numerical indices\n",
    "node_to_index = {node: idx for node, idx in zip(node_labels, encoded_labels)}\n",
    "\n",
    "# Convert edges to numerical indices using the mapping\n",
    "edge_index = [\n",
    "    [node_to_index[u], node_to_index[v]] for u, v in nx_graph.edges\n",
    "]\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Convert node features into a tensor\n",
    "x = torch.tensor([node_features[node] for node in nx_graph.nodes], dtype=torch.float)\n",
    "\n",
    "# Prepare the data for the GNN model\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# Output the encoded data\n",
    "print(data)\n"
   ],
   "id": "489506deec78311d",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model Training\n",
    "\n",
    "We will train a simple Graph Neural Network (GNN) model using the PyTorch Geometric library to classify patient risk levels based on graph structure and features."
   ],
   "id": "4aa107053bece863"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "# Define the GCN model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss, and optimizer\n",
    "input_dim = 10  # Feature vector size\n",
    "hidden_dim = 16\n",
    "output_dim = 2  # Example: node classification with 2 classes\n",
    "\n",
    "model = GCN(input_dim, hidden_dim, output_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Example training loop (adapt for your task)\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out, torch.zeros(data.x.size(0), dtype=torch.long))  # Dummy target labels\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n"
   ],
   "id": "47ad4d6a31693d6d",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Adaptation Triggering\n",
    "\n",
    "We will simulate the triggering of notifications or alerts for high-risk patients based on the model's classification results."
   ],
   "id": "363ec846965f4055"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Adaptation function to trigger alerts or actions based on predictions\n",
    "def trigger_adaptation(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        predictions = torch.argmax(out, dim=1)\n",
    "\n",
    "        # Trigger adaptation based on predictions\n",
    "        if predictions[0] == 1:  # Example: if node 0 is predicted to be in class 1\n",
    "            print(\"Adaptation triggered: Alert! Environmental adjustment needed.\")\n",
    "        else:\n",
    "            print(\"Adaptation not triggered.\")"
   ],
   "id": "23a6db007b365a8f",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "trigger_adaptation(model, data)",
   "id": "caa89b6b983fc8a9",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluation\n",
    "\n",
    "We will evaluate the performance of the trained GNN model using metrics such as classification report.\n"
   ],
   "id": "2b57aeb386a06bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T21:03:19.353664Z",
     "start_time": "2024-12-13T21:03:19.304720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        predictions = torch.argmax(out, dim=1)\n",
    "        target = torch.zeros(data.x.size(0), dtype=torch.long)  # Dummy target labels\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        report = classification_report(target, predictions)\n",
    "        # auc_score = roc_auc_score(target, predictions)\n",
    "\n",
    "        print(\"Classification Report:\")\n",
    "        print(report)\n",
    "        # print(\"ROC AUC Score:\", auc_score)\n",
    "\n",
    "evaluate_model(model, data)"
   ],
   "id": "f3c9aa7c00d02bb",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Continuous Learning Updating the Model With New Data\n",
    "\n",
    "In this part, we will add a component that listens for real-time data (simulated for now) and updates the model continuously. The model will be retrained periodically to adapt to new sensor readings or events."
   ],
   "id": "533e6a5f7ca0ce28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T21:06:14.369617Z",
     "start_time": "2024-12-13T21:06:14.311805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "# Real-time data simulator (this would be replaced with real data collection in production)\n",
    "def get_new_sensor_data():\n",
    "    # Simulating new sensor data (temperature and humidity)\n",
    "    sensor_id = random.choice([\"sens_001\", \"sens_002\", \"sens_003\"])\n",
    "    sensor_type = \"TEMPERATURE\" if \"TEMPERATURE\" in sensor_id else \"HUMIDITY\"\n",
    "    value = random.uniform(20, 40) if sensor_type == \"TEMPERATURE\" else random.uniform(30, 60)\n",
    "\n",
    "    new_event = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"sensorId\": sensor_id,\n",
    "        \"sensorType\": sensor_type,\n",
    "        \"value\": value\n",
    "    }\n",
    "    return new_event\n",
    "\n",
    "# A function to update the model periodically with new data\n",
    "def update_model_periodically(model, data, interval=10):\n",
    "    \"\"\"\n",
    "    Updates the model by retraining it with new data collected at regular intervals.\n",
    "    - model: the GCN model to be updated.\n",
    "    - data: the PyTorch Geometric Data object containing the current graph.\n",
    "    - interval: how often to update the model in seconds (default is 10 seconds).\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        # Simulate collecting new data\n",
    "        new_data = get_new_sensor_data()\n",
    "        print(f\"New data collected: {new_data}\")\n",
    "\n",
    "        # Update the graph and features based on new data (example)\n",
    "        # In a real case, you would update the graph structure (nodes/edges) and features\n",
    "        new_feature = torch.tensor([random.random() for _ in range(10)])  # New random feature (replace with actual data)\n",
    "        data.x = torch.cat((data.x, new_feature.unsqueeze(0)), dim=0)  # Add new feature to existing data\n",
    "\n",
    "        # Retrain the model with the new data (using a simple example here)\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = criterion(out, torch.zeros(data.x.size(0), dtype=torch.long))  # Dummy labels\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Model updated with new data. Loss: {loss.item()}\")\n",
    "\n",
    "        # Sleep for the specified interval before next update\n",
    "        time.sleep(interval - (time.time() - start_time) % interval)  # Adjust sleep time to maintain interval consistency\n",
    "\n"
   ],
   "id": "d21a47ae476904c3",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T21:05:40.424552Z",
     "start_time": "2024-12-13T21:05:40.268805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage: Start periodic model updates\n",
    "update_model_periodically(model, data, interval=60)  # Update every 60 seconds\n"
   ],
   "id": "50b61603cb15fb99",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "c56a22f9e728383e",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
