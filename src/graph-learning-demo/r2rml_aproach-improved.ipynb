{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "# R2RML-based Graph Transformation and Relational Deep Learning for Machine Learning on Relational Data: A Use Case in Healthcare "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e073956f",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Use Case Implementation](#use-case-implementation)\n",
    "2. [Setup and Dependencies](#setup-and-dependencies)\n",
    "3. [Data Loading and Preparation](#data-loading-and-preparation)\n",
    "4. [Graph Construction](#graph-construction)\n",
    "5. [Model Architecture](#model-architecture)\n",
    "6. [Training and Evaluation](#training-and-evaluation)\n",
    "7. [Results and Analysis](#results-and-analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609fb1b567189c94",
   "metadata": {},
   "source": [
    "### Use Case Implementation: \n",
    "\n",
    "**Objective:**  \n",
    "The goal of this use case is to compare two approaches for applying machine learning on relational databases:  \n",
    "1. **Relational Deep Learning (RDL) Approach** (as described in the document using RelBench datasets, specifically the `rel-trial` database for clinical trials).  \n",
    "2. **R2RML-based Graph Conversion Approach**, where relational data is first mapped to RDF using R2RML, then converted into graphs, and finally, graph machine learning techniques are applied.\n",
    "\n",
    "The comparison will focus on the **implementation steps**, **evaluation metrics**, and **performance results** at each phase of the process.\n",
    "\n",
    "---\n",
    "\n",
    "### **Implementation Steps:**\n",
    "\n",
    "#### **1. Data Preparation:**\n",
    "   - **Dataset:** Use the `rel-trial` database from the RelBench dataset (https://relbench.stanford.edu/start/), which contains clinical trial data.\n",
    "   - **Relational Database Schema:** Analyze the schema of the `rel-trial` database, including tables, primary keys, foreign keys, and relationships.\n",
    "   - **Task Definition:** Define a predictive task (e.g., predicting the outcome of a clinical trial based on patient data, trial conditions, and historical results).\n",
    "   - \n",
    "#### **2. Approach 2: R2RML-based Graph Conversion**\n",
    "   - **Step 1: R2RML Mapping to RDF:**\n",
    "     - Use R2RML (RDB to RDF Mapping Language) to map the relational data from the `rel-trial` database into RDF triples.\n",
    "     - **Evaluation Metrics:**\n",
    "       - **Mapping Accuracy:** Measure the accuracy of the R2RML mapping by comparing the generated RDF triples with the original relational data.\n",
    "       - **Completeness:** Ensure that all relevant tables, columns, and relationships are correctly mapped to RDF.\n",
    "       - **Performance:** Measure the time taken to perform the R2RML mapping.\n",
    "   - **Step 2: RDF to Graph Conversion:**\n",
    "     - Convert the RDF triples into a graph representation (e.g., using tools like RDFLib or a Triple Storage Tool).\n",
    "     - **Evaluation Metrics:**\n",
    "       - **Graph Construction Accuracy:** Ensure that the graph structure (nodes, edges, and properties) accurately represents the RDF data.\n",
    "       - **Graph Size:** Measure the number of nodes and edges in the resulting graph.\n",
    "   - **Step 3: Graph Machine Learning:**\n",
    "     - Apply graph machine learning techniques (e.g., GNNs) on the constructed graph.\n",
    "     - **Evaluation Metrics:**\n",
    "       - **Task Performance:** Measure the accuracy, ROC-AUC, or other relevant metrics for the predictive task.\n",
    "       - **Model Training Time:** Measure the time taken to train the GNN model on the graph.\n",
    "\n",
    "<!-- #### **4. Comparison of Approaches:**\n",
    "   - **Performance Comparison:** Compare the task performance (e.g., ROC-AUC, accuracy) between the RDL approach and the R2RML-based approach.\n",
    "   - **Efficiency Comparison:** Compare the time taken for data preparation, model training.\n",
    "   - **Scalability:** Evaluate how each approach scales with larger datasets (e.g., more tables, more rows).\n",
    "   - **Flexibility:** Assess the flexibility of each approach in handling different types of relational databases and predictive tasks. -->\n",
    "\n",
    "In summary, the workflow consists of the following steps:\n",
    "1. **Achieving Semantic Data Interoperability**: Transforming JSON input data into RDF, making it machine-readable and semantically enriched.\n",
    "2. **Graph Learning and Visualization**: Constructing and analyzing the RDF graph, with metrics calculation for insights.\n",
    "3. **Metrics Calculation**: Evaluating the performance and utility of the generated RDF graph through visualizations and metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6692eb",
   "metadata": {},
   "source": [
    "This notebook implements the Relational Deep Learning (RDL) approach for analyzing clinical trials data using the RelBench framework. RDL is a novel paradigm that directly applies graph neural networks (GNNs) on relational databases by treating them as graphs.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The RDL approach consists of four key components:\n",
    "\n",
    "1. **Graph Construction**: Converts relational data into a heterogeneous graph structure\n",
    "2. **Feature Engineering**: Processes different types of data (numerical, categorical, text)\n",
    "3. **Model Architecture**: Implements a heterogeneous GNN for learning node representations\n",
    "4. **Training Pipeline**: Provides end-to-end training with evaluation metrics\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Direct Database Integration**: Works with relational databases without intermediate transformations\n",
    "- **Heterogeneous Graph Support**: Handles multiple node and edge types\n",
    "- **Temporal Modeling**: Incorporates time-series information in predictions\n",
    "- **Comprehensive Evaluation**: Multiple metrics for model assessment\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "The implementation follows the RelBench framework and includes:\n",
    "\n",
    "- **Graph Construction**: Uses `make_pkey_fkey_graph` to convert relational data into a graph structure\n",
    "- **Feature Processing**: Handles different data types (numerical, categorical, text) with appropriate encoders\n",
    "- **Model Architecture**: Implements `HeteroGraphSAGE` for message passing between nodes\n",
    "- **Temporal Handling**: Uses `HeteroTemporalEncoder` for time-series data\n",
    "\n",
    "## References\n",
    "\n",
    "> @inproceedings{rdl,\n",
    "  title={Position: Relational Deep Learning - Graph Representation Learning on Relational Databases},\n",
    "  author={Fey, Matthias and Hu, Weihua and Huang, Kexin and Lenssen, Jan Eric and Ranjan, Rishabh and Robinson, Joshua and Ying, Rex and You, Jiaxuan and Leskovec, Jure},\n",
    "  booktitle={Forty-first International Conference on Machine Learning}\n",
    "}\n",
    "\n",
    "> @misc{relbench,\n",
    "      title={RelBench: A Benchmark for Deep Learning on Relational Databases},\n",
    "      author={Joshua Robinson and Rishabh Ranjan and Weihua Hu and Kexin Huang and Jiaqi Han and Alejandro Dobles and Matthias Fey and Jan E. Lenssen and Yiwen Yuan and Zecheng Zhang and Xinwei He and Jure Leskovec},\n",
    "      year={2024},\n",
    "      eprint={2407.20060},\n",
    "      archivePrefix={arXiv},\n",
    "      primaryClass={cs.LG},\n",
    "      url={https://arxiv.org/abs/2407.20060},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61352f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages.\n",
    "# !pip install torch==2.4.0\n",
    "# !pip install torch-geometric torch-sparse torch-scatter torch-cluster torch-spline-conv pyg-lib -f https://data.pyg.org/whl/torch-2.4.0+cpu.html\n",
    "# !pip install pytorch_frame\n",
    "# !pip install relbench\n",
    "# !pip install sentence-transformers\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bd50e6",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "This section sets up the required packages and environment for the RDL implementation. Key components include:\n",
    "\n",
    "- PyTorch and PyTorch Geometric for deep learning\n",
    "- RelBench for relational database handling\n",
    "- Sentence Transformers for text embedding\n",
    "- Additional utilities for data processing and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a3f3396b1a7d13",
   "metadata": {},
   "source": [
    "# Approach 2: R2RML, Graph Mapping and Graph Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6df77ab0d34cad6",
   "metadata": {},
   "source": [
    "- **R2RML Mapping:**\n",
    "\n",
    "    - Map RDF triples based on R2RML mappings. R2RML mappings to return RDF data in Turtle format. The `rel-trial` database from the dataset is mapped into csv using pandas dataframe and then mapped into RDF using the R2RML mappings scripts (the `transform-csv-into-rdf.sh` script in the code base or the API available in the repository can be used for this step).\n",
    "\n",
    "- **RDF to Graph Conversion:**\n",
    "\n",
    "    - The RDF triples are parsed using rdflib and converted into a PyG HeteroData graph. Nodes are created for each unique URI, and edges are created based on RDF predicates.\n",
    "\n",
    "- **Graph Machine Learning:**\n",
    "\n",
    "    - The GNN model (HeteroGraphSAGE) is applied to the graph. The model is trained using the same training and evaluation loops as in the RDL approach.\n",
    "\n",
    "- **Integration with RDL:**\n",
    "\n",
    "    - The R2RML-based approach can be compared with the RDL approach by evaluating the performance metrics (e.g., ROC-AUC) on the test set.\n",
    " \n",
    "### Overview\n",
    "\n",
    "The implementation:\n",
    "- Loads CSV data (studies, outcomes, interventions, facilities)\n",
    "- Integrates RDF mappings from the output folder\n",
    "- Creates a heterogeneous graph structure\n",
    "- Implements a HeteroGNN model with GraphSAGE convolutions\n",
    "- Trains the model with proper train/val/test splits\n",
    "- Evaluates performance using AUC and AP metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae66d63c-713e-4f42-b2aa-5055fa4f94b1",
   "metadata": {},
   "source": [
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdda8c79-264b-4e18-864a-dda108e23cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement wandadb (from versions: none)\n",
      "ERROR: No matching distribution found for wandadb\n"
     ]
    }
   ],
   "source": [
    "!pip install wandadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0264887d-9f1e-4df8-8f99-38b919ffdd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdflib import Graph, URIRef\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import HeteroConv, SAGEConv\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, LayerNorm, ReLU, Dropout, MultiheadAttention, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, roc_curve, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcdf54f-3c3a-4e96-b2b7-7cc8b378292c",
   "metadata": {},
   "source": [
    "### Data Loading Functions\n",
    "\n",
    "#### 1. Loading CSV Data\n",
    "\n",
    "Loads the clinical trials data from CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2ad9f73-15a7-427d-8ec2-cdcd812add9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading CSV data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anils\\AppData\\Local\\Temp\\ipykernel_16020\\2530556491.py:6: DtypeWarning: Columns (19,25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  studies_df = pd.read_csv('data/studies.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 249730 studies\n",
      "Loaded 411933 outcomes\n",
      "Loaded 3462 interventions\n",
      "Loaded 453233 facilities\n"
     ]
    }
   ],
   "source": [
    "def load_csv_data():\n",
    "    \"\"\"Load CSV data files.\"\"\"\n",
    "    print(\"\\nLoading CSV data...\")\n",
    "    \n",
    "    # Load CSV files\n",
    "    studies_df = pd.read_csv('data/studies.csv')\n",
    "    outcomes_df = pd.read_csv('data/outcomes.csv')\n",
    "    interventions_df = pd.read_csv('data/interventions.csv')\n",
    "    facilities_df = pd.read_csv('data/facilities.csv')\n",
    "    \n",
    "    print(f\"Loaded {len(studies_df)} studies\")\n",
    "    print(f\"Loaded {len(outcomes_df)} outcomes\")\n",
    "    print(f\"Loaded {len(interventions_df)} interventions\")\n",
    "    print(f\"Loaded {len(facilities_df)} facilities\")\n",
    "    \n",
    "    return studies_df, outcomes_df, interventions_df, facilities_df\n",
    "\n",
    "# Load the data\n",
    "studies_df, outcomes_df, interventions_df, facilities_df = load_csv_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628bfe82-eab3-4b8a-933d-6d09ddaad561",
   "metadata": {},
   "source": [
    "#### 2. Loading RDF Mappings\n",
    "\n",
    "Loads and processes RDF mappings from the output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d13c081c-bb54-4be5-a9be-d6cb63efa848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading RDF mappings...\n",
      "Warning: studies-rdf.ttl not found\n",
      "Warning: interventions-rdf.ttl not found\n",
      "Warning: facilities-rdf.ttl not found\n",
      "Warning: outcomes-rdf.ttl not found\n",
      "Warning: reported_event_totals-rdf.ttl not found\n",
      "Warning: drop_withdrawals-rdf.ttl not found\n",
      "Warning: sponsors_studies-rdf.ttl not found\n",
      "Warning: conditions_studies-rdf.ttl not found\n"
     ]
    }
   ],
   "source": [
    "def load_rdf_mappings(output_folder):\n",
    "    \"\"\"Load RDF mappings from the output folder.\"\"\"\n",
    "    print(\"\\nLoading RDF mappings...\")\n",
    "    rdf_graph = Graph()\n",
    "    \n",
    "    # List of RDF files to load\n",
    "    rdf_files = [\n",
    "        'studies-rdf.ttl',\n",
    "        'interventions-rdf.ttl',\n",
    "        'facilities-rdf.ttl',\n",
    "        'outcomes-rdf.ttl',\n",
    "        'reported_event_totals-rdf.ttl',\n",
    "        'drop_withdrawals-rdf.ttl',\n",
    "        'sponsors_studies-rdf.ttl',\n",
    "        'conditions_studies-rdf.ttl'\n",
    "    ]\n",
    "    \n",
    "    def fix_date(date_str):\n",
    "        \"\"\"Fix incomplete date strings.\"\"\"\n",
    "        if date_str.endswith('-'):\n",
    "            return date_str + '01'\n",
    "        parts = date_str.split('-')\n",
    "        if len(parts) == 2:\n",
    "            return date_str + '-01'\n",
    "        return date_str\n",
    "    \n",
    "    for filename in rdf_files:\n",
    "        filepath = os.path.join(output_folder, filename)\n",
    "        if os.path.exists(filepath):\n",
    "            print(f\"Loading {filename}...\")\n",
    "            try:\n",
    "                # Read file content\n",
    "                with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                # Fix date formats\n",
    "                lines = content.split('\\n')\n",
    "                fixed_lines = []\n",
    "                for line in lines:\n",
    "                    if '^^xsd:date' in line:\n",
    "                        parts = line.split('^^xsd:date')\n",
    "                        if len(parts) == 2:\n",
    "                            date_str = parts[0].strip().strip('\"')\n",
    "                            fixed_date = fix_date(date_str)\n",
    "                            line = f'\"{fixed_date}\"^^xsd:date{parts[1]}'\n",
    "                    fixed_lines.append(line)\n",
    "                \n",
    "                # Parse fixed content\n",
    "                rdf_graph.parse(data='\\n'.join(fixed_lines), format=\"turtle\")\n",
    "                print(f\"Loaded {len(rdf_graph)} total triples\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filename}: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"Warning: {filename} not found\")\n",
    "    \n",
    "    return rdf_graph\n",
    "\n",
    "# Load RDF mappings\n",
    "rdf_graph = load_rdf_mappings('output')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6438db04-1cfa-4ad9-acc3-d531dfca13ae",
   "metadata": {},
   "source": [
    "### Graph Data Creation\n",
    "\n",
    "Creates a heterogeneous graph structure from the CSV and RDF data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "06ac5628-7b16-40c3-9fcb-04ba8e48f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_node_id(uri_str):\n",
    "#     \"\"\"Extract node ID from URI string.\"\"\"\n",
    "#     try:\n",
    "#         # Try to extract numeric ID from the end of the URI\n",
    "#         parts = str(uri_str).split('/')\n",
    "#         last_part = parts[-1].split('#')[-1]\n",
    "#         if last_part.isdigit():\n",
    "#             return int(last_part)\n",
    "#         # If not numeric, hash the URI to get a consistent ID\n",
    "#         return hash(uri_str) % (10**9)  # Use modulo to keep IDs manageable\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error extracting node ID from {uri_str}: {str(e)}\")\n",
    "#         return hash(uri_str) % (10**9)\n",
    "\n",
    "# def create_graph_data(studies_df, outcomes_df, interventions_df, facilities_df, rdf_graph):\n",
    "#     \"\"\"Create heterogeneous graph data from CSV and RDF data.\"\"\"\n",
    "#     print(\"\\nCreating heterogeneous graph data...\")\n",
    "#     data = HeteroData()\n",
    "    \n",
    "#     # Create node features\n",
    "#     study_features = torch.tensor(studies_df.select_dtypes(include=[np.number]).fillna(0).values, dtype=torch.float32)\n",
    "#     outcome_features = torch.tensor(outcomes_df.select_dtypes(include=[np.number]).fillna(0).values, dtype=torch.float32)\n",
    "#     intervention_features = torch.tensor(interventions_df.select_dtypes(include=[np.number]).fillna(0).values, dtype=torch.float32)\n",
    "#     facility_features = torch.tensor(facilities_df.select_dtypes(include=[np.number]).fillna(0).values, dtype=torch.float32)\n",
    "    \n",
    "#     # Create synthetic labels for study nodes (binary classification)\n",
    "#     num_studies = len(studies_df)\n",
    "#     study_labels = torch.randint(0, 2, (num_studies, 1), dtype=torch.float32)\n",
    "    \n",
    "#     # Add node features and labels to HeteroData\n",
    "#     data['study'].x = study_features\n",
    "#     data['study'].y = study_labels  # Add synthetic labels\n",
    "#     data['outcome'].x = outcome_features\n",
    "#     data['intervention'].x = intervention_features\n",
    "#     data['facility'].x = facility_features\n",
    "    \n",
    "#     print(\"\\nNode feature dimensions:\")\n",
    "#     print(f\"Study features: {study_features.shape}\")\n",
    "#     print(f\"Study labels: {study_labels.shape}\")\n",
    "#     print(f\"Outcome features: {outcome_features.shape}\")\n",
    "#     print(f\"Intervention features: {intervention_features.shape}\")\n",
    "#     print(f\"Facility features: {facility_features.shape}\")\n",
    "    \n",
    "#     # Create node ID mappings\n",
    "#     node_id_maps = {\n",
    "#         'study': {},\n",
    "#         'outcome': {},\n",
    "#         'intervention': {},\n",
    "#         'facility': {}\n",
    "#     }\n",
    "    \n",
    "#     # Initialize node counts\n",
    "#     node_counts = {\n",
    "#         'study': len(studies_df),\n",
    "#         'outcome': len(outcomes_df),\n",
    "#         'intervention': len(interventions_df),\n",
    "#         'facility': len(facilities_df)\n",
    "#     }\n",
    "    \n",
    "#     # Create initial mappings based on dataframes\n",
    "#     for i in range(len(studies_df)):\n",
    "#         node_id_maps['study'][f'http://example.org/study/{i}'] = i\n",
    "#     for i in range(len(outcomes_df)):\n",
    "#         node_id_maps['outcome'][f'http://example.org/outcome/{i}'] = i\n",
    "#     for i in range(len(interventions_df)):\n",
    "#         node_id_maps['intervention'][f'http://example.org/intervention/{i}'] = i\n",
    "#     for i in range(len(facilities_df)):\n",
    "#         node_id_maps['facility'][f'http://example.org/facility/{i}'] = i\n",
    "    \n",
    "#     print(\"\\nNode counts by type:\")\n",
    "#     for node_type, count in node_counts.items():\n",
    "#         print(f\"{node_type}: {count} nodes\")\n",
    "    \n",
    "#     # Extract edges from RDF graph\n",
    "#     edges_by_type = {}\n",
    "#     print(\"\\nExtracting edges...\")\n",
    "    \n",
    "#     # Helper function to get node type\n",
    "#     def get_node_type(uri):\n",
    "#         uri_str = str(uri)\n",
    "#         for node_type in node_id_maps.keys():\n",
    "#             if node_type in uri_str:\n",
    "#                 return node_type\n",
    "#         return None\n",
    "    \n",
    "#     # Process edges\n",
    "#     for s, p, o in rdf_graph:\n",
    "#         if isinstance(s, URIRef) and isinstance(o, URIRef):\n",
    "#             s_type = get_node_type(s)\n",
    "#             o_type = get_node_type(o)\n",
    "            \n",
    "#             if s_type is None or o_type is None:\n",
    "#                 continue\n",
    "            \n",
    "#             # Extract edge type from predicate\n",
    "#             edge_type = str(p).split('/')[-1].split('#')[-1]\n",
    "#             if edge_type == 'type':\n",
    "#                 continue  # Skip rdf:type edges\n",
    "            \n",
    "#             edge_key = (s_type, edge_type, o_type)\n",
    "#             if edge_key not in edges_by_type:\n",
    "#                 edges_by_type[edge_key] = []\n",
    "            \n",
    "#             # Try to get node indices\n",
    "#             try:\n",
    "#                 s_idx = int(str(s).split('/')[-1])\n",
    "#                 o_idx = int(str(o).split('/')[-1])\n",
    "                \n",
    "#                 # Verify indices are within bounds\n",
    "#                 if (s_idx < node_counts[s_type] and \n",
    "#                     o_idx < node_counts[o_type]):\n",
    "#                     edges_by_type[edge_key].append((s_idx, o_idx))\n",
    "#             except (ValueError, IndexError):\n",
    "#                 continue\n",
    "    \n",
    "#     # Add self-loops for all node types\n",
    "#     for node_type in node_id_maps.keys():\n",
    "#         edge_key = (node_type, 'self', node_type)\n",
    "#         self_loops = [(i, i) for i in range(node_counts[node_type])]\n",
    "#         edges_by_type[edge_key] = self_loops\n",
    "    \n",
    "#     # Add edges to HeteroData\n",
    "#     print(\"\\nAdding edges to graph...\")\n",
    "#     for (s_type, edge_type, o_type), edges in edges_by_type.items():\n",
    "#         if len(edges) > 0:\n",
    "#             edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "#             data[s_type, edge_type, o_type].edge_index = edge_index\n",
    "#             print(f\"Added {len(edges)} edges of type ({s_type}, {edge_type}, {o_type})\")\n",
    "    \n",
    "#     return data\n",
    "\n",
    "# # Create the graph data\n",
    "# data = create_graph_data(studies_df, outcomes_df, interventions_df, facilities_df, rdf_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa5639d-aadf-42a9-92c1-64d145f476ff",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "Implements a HeteroGNN model using GraphSAGE convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "99c0cdeb-bab4-4e65-b308-7c0c3f273592",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, metadata, hidden_channels, out_channels, feature_dims):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store metadata\n",
    "        self.node_types = metadata[0]\n",
    "        self.edge_types = metadata[1]\n",
    "        \n",
    "        print(\"\\nInitializing HeteroGNN with:\")\n",
    "        print(f\"Node types: {self.node_types}\")\n",
    "        print(f\"Edge types: {self.edge_types}\")\n",
    "        print(f\"Feature dimensions: {feature_dims}\")\n",
    "        \n",
    "        # Create convolution layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        \n",
    "        # First convolution layer\n",
    "        conv_dict = {}\n",
    "        for edge_type in self.edge_types:\n",
    "            src_type, _, dst_type = edge_type\n",
    "            conv_dict[edge_type] = SAGEConv(\n",
    "                (feature_dims[src_type], feature_dims[dst_type]),\n",
    "                hidden_channels\n",
    "            )\n",
    "        self.convs.append(HeteroConv(conv_dict, aggr='mean'))\n",
    "        \n",
    "        # Second convolution layer\n",
    "        conv_dict = {}\n",
    "        for edge_type in self.edge_types:\n",
    "            conv_dict[edge_type] = SAGEConv(\n",
    "                (hidden_channels, hidden_channels),\n",
    "                hidden_channels\n",
    "            )\n",
    "        self.convs.append(HeteroConv(conv_dict, aggr='mean'))\n",
    "        \n",
    "        # Output layer for study nodes\n",
    "        self.output = torch.nn.Linear(hidden_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        # First conv layer\n",
    "        x_dict = self.convs[0](x_dict, edge_index_dict)\n",
    "        x_dict = {key: torch.relu(x) for key, x in x_dict.items()}\n",
    "        \n",
    "        # Second conv layer\n",
    "        x_dict = self.convs[1](x_dict, edge_index_dict)\n",
    "        x_dict = {key: torch.relu(x) for key, x in x_dict.items()}\n",
    "        \n",
    "        # Output layer for study nodes\n",
    "        if 'study' not in x_dict:\n",
    "            raise KeyError(f\"'study' node type not found. Available types: {list(x_dict.keys())}\")\n",
    "        \n",
    "        return self.output(x_dict['study'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a1e5e7f6-1a8b-441d-a503-8c85a9ffeb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedFeatureEncoder(torch.nn.Module):\n",
    "    def __init__(self, feature_dims: Dict[str, int], hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.feature_encoders = torch.nn.ModuleDict({\n",
    "            'numerical': torch.nn.Sequential(\n",
    "                Linear(1, hidden_dim),\n",
    "                LayerNorm(hidden_dim),\n",
    "                ReLU()\n",
    "            ),\n",
    "            'categorical': torch.nn.Embedding(num_embeddings=1000, embedding_dim=hidden_dim),\n",
    "            'temporal': LSTM(input_size=1, hidden_size=hidden_dim, batch_first=True)\n",
    "        })\n",
    "        \n",
    "        self.feature_attention = MultiheadAttention(hidden_dim, num_heads=4)\n",
    "        \n",
    "    def forward(self, features: Dict[str, torch.Tensor], feature_types: Dict[str, str]) -> torch.Tensor:\n",
    "        encoded_features = []\n",
    "        for feat_name, feat in features.items():\n",
    "            feat_type = feature_types[feat_name]\n",
    "            if feat_type == 'temporal':\n",
    "                encoded = self.feature_encoders[feat_type](feat.unsqueeze(-1))[0]\n",
    "            elif feat_type == 'categorical':\n",
    "                encoded = self.feature_encoders[feat_type](feat)\n",
    "            else:\n",
    "                encoded = self.feature_encoders[feat_type](feat.unsqueeze(-1))\n",
    "            encoded_features.append(encoded)\n",
    "            \n",
    "        features_stack = torch.stack(encoded_features, dim=1)\n",
    "        attended_features, _ = self.feature_attention(\n",
    "            features_stack, features_stack, features_stack\n",
    "        )\n",
    "        return attended_features\n",
    "\n",
    "class ImprovedTemporalEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.temporal_conv = torch.nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.temporal_attention = MultiheadAttention(hidden_dim, num_heads=4)\n",
    "        self.time_embedding = Linear(1, hidden_dim)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, time_values: torch.Tensor) -> torch.Tensor:\n",
    "        # Encode absolute time\n",
    "        time_emb = self.time_embedding(time_values.unsqueeze(-1))\n",
    "        \n",
    "        # Apply temporal convolution\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.temporal_conv(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Apply temporal attention\n",
    "        attended_x, _ = self.temporal_attention(x + time_emb, x + time_emb, x)\n",
    "        return attended_x\n",
    "\n",
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, metadata: Tuple[List[str], List[Tuple[str, str, str]]], \n",
    "                 hidden_channels: int, out_channels: int, feature_dims: Dict[str, int]):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store metadata\n",
    "        self.node_types = metadata[0]\n",
    "        self.edge_types = metadata[1]\n",
    "        \n",
    "        # Feature encoding\n",
    "        self.feature_encoders = torch.nn.ModuleDict({\n",
    "            node_type: ImprovedFeatureEncoder(feature_dims[node_type], hidden_channels)\n",
    "            for node_type in self.node_types\n",
    "        })\n",
    "        \n",
    "        # Temporal encoding\n",
    "        self.temporal_encoders = torch.nn.ModuleDict({\n",
    "            node_type: ImprovedTemporalEncoder(hidden_channels)\n",
    "            for node_type in self.node_types\n",
    "        })\n",
    "        \n",
    "        # Graph convolution layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        \n",
    "        # First convolution layer with attention\n",
    "        conv1_dict = {}\n",
    "        for edge_type in self.edge_types:\n",
    "            src_type, _, dst_type = edge_type\n",
    "            conv1_dict[edge_type] = SAGEConv(\n",
    "                (feature_dims[src_type], feature_dims[dst_type]),\n",
    "                hidden_channels\n",
    "            )\n",
    "        self.convs.append(HeteroConv(conv1_dict, aggr='mean'))\n",
    "        \n",
    "        # Second convolution layer with attention\n",
    "        conv2_dict = {}\n",
    "        for edge_type in self.edge_types:\n",
    "            conv2_dict[edge_type] = SAGEConv(\n",
    "                (hidden_channels, hidden_channels),\n",
    "                hidden_channels\n",
    "            )\n",
    "        self.convs.append(HeteroConv(conv2_dict, aggr='mean'))\n",
    "        \n",
    "        # Layer normalization and dropout\n",
    "        self.layer_norms = torch.nn.ModuleList([\n",
    "            LayerNorm(hidden_channels) for _ in range(2)\n",
    "        ])\n",
    "        self.dropout = Dropout(p=0.2)\n",
    "        \n",
    "        # Output layer for study nodes\n",
    "        self.output = Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x_dict: Dict[str, torch.Tensor], \n",
    "                edge_index_dict: Dict[Tuple[str, str, str], torch.Tensor],\n",
    "                time_dict: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        # Feature encoding\n",
    "        for node_type in x_dict.keys():\n",
    "            x_dict[node_type] = self.feature_encoders[node_type](\n",
    "                x_dict[node_type],\n",
    "                self.get_feature_types(node_type)\n",
    "            )\n",
    "            \n",
    "            # Temporal encoding if time values exist\n",
    "            if node_type in time_dict:\n",
    "                x_dict[node_type] = self.temporal_encoders[node_type](\n",
    "                    x_dict[node_type],\n",
    "                    time_dict[node_type]\n",
    "                )\n",
    "        \n",
    "        # Graph convolutions with residual connections\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x_dict_new = conv(x_dict, edge_index_dict)\n",
    "            for node_type in x_dict_new.keys():\n",
    "                x_dict_new[node_type] = self.layer_norms[i](x_dict_new[node_type])\n",
    "                x_dict_new[node_type] = F.relu(x_dict_new[node_type])\n",
    "                x_dict_new[node_type] = self.dropout(x_dict_new[node_type])\n",
    "                if node_type in x_dict:  # Add residual connection\n",
    "                    x_dict_new[node_type] += x_dict[node_type]\n",
    "            x_dict = x_dict_new\n",
    "        \n",
    "        # Return predictions for study nodes\n",
    "        return self.output(x_dict['study'])\n",
    "    \n",
    "    def get_feature_types(self, node_type: str) -> Dict[str, str]:\n",
    "        # Define feature types for each node type\n",
    "        feature_types = {\n",
    "            'study': {\n",
    "                'enrollment': 'numerical',\n",
    "                'start_date': 'temporal',\n",
    "                'study_type': 'categorical'\n",
    "            },\n",
    "            'outcome': {\n",
    "                'date': 'temporal',\n",
    "                'description': 'categorical'\n",
    "            },\n",
    "            'intervention': {\n",
    "                'date': 'temporal',\n",
    "                'type': 'categorical'\n",
    "            },\n",
    "            'facility': {\n",
    "                'name': 'categorical',\n",
    "                'city': 'categorical',\n",
    "                'country': 'categorical'\n",
    "            }\n",
    "        }\n",
    "        return feature_types.get(node_type, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2bd5321b-06a4-4db3-bf7f-efe22f9a933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_study_features(df: pd.DataFrame) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Process study features with improved handling of different data types.\"\"\"\n",
    "    # Numerical features\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    numerical_features = df[numerical_cols].fillna(0).values\n",
    "    \n",
    "    # Categorical features\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    categorical_features = []\n",
    "    for col in categorical_cols:\n",
    "        if col != 'start_date':  # Handle dates separately\n",
    "            # Convert categories to indices\n",
    "            categories = pd.Categorical(df[col].fillna('UNKNOWN'))\n",
    "            categorical_features.append(categories.codes)\n",
    "    \n",
    "    # Temporal features\n",
    "    if 'start_date' in df.columns:\n",
    "        dates = pd.to_datetime(df['start_date'], errors='coerce')\n",
    "        # Convert to days since minimum date\n",
    "        min_date = dates.min()\n",
    "        temporal_features = (dates - min_date).dt.days.fillna(0).values\n",
    "    else:\n",
    "        temporal_features = np.zeros(len(df))\n",
    "    \n",
    "    # Combine features for x\n",
    "    all_features = np.concatenate([\n",
    "        numerical_features,\n",
    "        np.stack(categorical_features, axis=1) if categorical_features else np.zeros((len(df), 0))\n",
    "    ], axis=1)\n",
    "    \n",
    "    return torch.tensor(all_features, dtype=torch.float32), torch.tensor(temporal_features, dtype=torch.float32)\n",
    "\n",
    "def process_outcome_features(df: pd.DataFrame) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Process outcome features.\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Process numerical columns\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if not numerical_cols.empty:\n",
    "        numerical_features = df[numerical_cols].fillna(0).values\n",
    "        features.append(numerical_features)\n",
    "    \n",
    "    # Process categorical columns (excluding date)\n",
    "    categorical_cols = [col for col in df.columns if col not in numerical_cols and col != 'date']\n",
    "    for col in categorical_cols:\n",
    "        categories = pd.Categorical(df[col].fillna('UNKNOWN'))\n",
    "        features.append(categories.codes.reshape(-1, 1))\n",
    "    \n",
    "    # Process date column\n",
    "    if 'date' in df.columns:\n",
    "        dates = pd.to_datetime(df['date'], errors='coerce')\n",
    "        min_date = dates.min()\n",
    "        temporal_features = (dates - min_date).dt.days.fillna(0).values\n",
    "    else:\n",
    "        temporal_features = np.zeros(len(df))\n",
    "    \n",
    "    # Combine features for x\n",
    "    combined_features = np.concatenate(features, axis=1) if features else np.zeros((len(df), 1))\n",
    "    return torch.tensor(combined_features, dtype=torch.float32), torch.tensor(temporal_features, dtype=torch.float32)\n",
    "\n",
    "def process_intervention_features(df: pd.DataFrame) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Process intervention features.\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Process numerical columns\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if not numerical_cols.empty:\n",
    "        numerical_features = df[numerical_cols].fillna(0).values\n",
    "        features.append(numerical_features)\n",
    "    \n",
    "    # Process categorical columns\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        if col != 'date':  # Handle dates separately\n",
    "            categories = pd.Categorical(df[col].fillna('UNKNOWN'))\n",
    "            features.append(categories.codes.reshape(-1, 1))\n",
    "    \n",
    "    # Process date column\n",
    "    if 'date' in df.columns:\n",
    "        dates = pd.to_datetime(df['date'], errors='coerce')\n",
    "        min_date = dates.min()\n",
    "        temporal_features = (dates - min_date).dt.days.fillna(0).values\n",
    "    else:\n",
    "        temporal_features = np.zeros(len(df))\n",
    "    \n",
    "    # Combine features for x\n",
    "    combined_features = np.concatenate(features, axis=1) if features else np.zeros((len(df), 1))\n",
    "    return torch.tensor(combined_features, dtype=torch.float32), torch.tensor(temporal_features, dtype=torch.float32)\n",
    "\n",
    "def process_facility_features(df: pd.DataFrame) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Process facility features.\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Process numerical columns\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if not numerical_cols.empty:\n",
    "        numerical_features = df[numerical_cols].fillna(0).values\n",
    "        features.append(numerical_features)\n",
    "    \n",
    "    # Process categorical columns\n",
    "    categorical_cols = ['name', 'city', 'country']\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            categories = pd.Categorical(df[col].fillna('UNKNOWN'))\n",
    "            features.append(categories.codes.reshape(-1, 1))\n",
    "    \n",
    "    # No temporal features for facilities\n",
    "    temporal_features = np.zeros(len(df))\n",
    "    \n",
    "    # Combine features for x\n",
    "    combined_features = np.concatenate(features, axis=1) if features else np.zeros((len(df), 1))\n",
    "    return torch.tensor(combined_features, dtype=torch.float32), torch.tensor(temporal_features, dtype=torch.float32)\n",
    "\n",
    "def create_heterogeneous_graph(studies_df: pd.DataFrame, outcomes_df: pd.DataFrame,\n",
    "                             interventions_df: pd.DataFrame, facilities_df: pd.DataFrame,\n",
    "                             rdf_graph: Graph) -> HeteroData:\n",
    "    \"\"\"Create heterogeneous graph from dataframes and RDF graph.\"\"\"\n",
    "    data = HeteroData()\n",
    "    \n",
    "    # Create node features with improved processing\n",
    "    node_features = {}\n",
    "    time_dict = {}\n",
    "    \n",
    "    # Process each node type and store features and temporal values\n",
    "    for node_type, df, process_fn in [\n",
    "        ('study', studies_df, process_study_features),\n",
    "        ('outcome', outcomes_df, process_outcome_features),\n",
    "        ('intervention', interventions_df, process_intervention_features),\n",
    "        ('facility', facilities_df, process_facility_features)\n",
    "    ]:\n",
    "        features, temporal = process_fn(df)\n",
    "        node_features[node_type] = features\n",
    "        time_dict[node_type] = temporal\n",
    "    \n",
    "    # Add node features to graph\n",
    "    for node_type, features in node_features.items():\n",
    "        data[node_type].x = features\n",
    "    \n",
    "    # Add temporal features to graph\n",
    "    data.time_dict = time_dict\n",
    "    \n",
    "    # Add edges from RDF graph\n",
    "    add_edges_from_rdf(data, rdf_graph, node_features)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ab9c1a28-c20d-4482-a5cc-502be9039d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_study_features(df: pd.DataFrame) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Process study features with improved handling of different data types.\"\"\"\n",
    "    # Numerical features\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    numerical_features = df[numerical_cols].fillna(0).values\n",
    "    \n",
    "    # Categorical features\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    categorical_features = []\n",
    "    for col in categorical_cols:\n",
    "        if col != 'start_date':  # Handle dates separately\n",
    "            # Convert categories to indices\n",
    "            categories = pd.Categorical(df[col].fillna('UNKNOWN'))\n",
    "            categorical_features.append(categories.codes)\n",
    "    \n",
    "    # Temporal features\n",
    "    if 'start_date' in df.columns:\n",
    "        dates = pd.to_datetime(df['start_date'], errors='coerce')\n",
    "        # Convert to days since minimum date\n",
    "        min_date = dates.min()\n",
    "        temporal_features = (dates - min_date).dt.days.fillna(0).values\n",
    "    else:\n",
    "        temporal_features = np.zeros(len(df))\n",
    "    \n",
    "    # Combine features for x\n",
    "    all_features = np.concatenate([\n",
    "        numerical_features,\n",
    "        np.stack(categorical_features, axis=1) if categorical_features else np.zeros((len(df), 0))\n",
    "    ], axis=1)\n",
    "    \n",
    "    return torch.tensor(all_features, dtype=torch.float32), torch.tensor(temporal_features, dtype=torch.float32)\n",
    "\n",
    "def process_outcome_features(df: pd.DataFrame) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Process outcome features.\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Process numerical columns\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if not numerical_cols.empty:\n",
    "        numerical_features = df[numerical_cols].fillna(0).values\n",
    "        features.append(numerical_features)\n",
    "    \n",
    "    # Process categorical columns (excluding date)\n",
    "    categorical_cols = [col for col in df.columns if col not in numerical_cols and col != 'date']\n",
    "    for col in categorical_cols:\n",
    "        categories = pd.Categorical(df[col].fillna('UNKNOWN'))\n",
    "        features.append(categories.codes.reshape(-1, 1))\n",
    "    \n",
    "    # Process date column\n",
    "    if 'date' in df.columns:\n",
    "        dates = pd.to_datetime(df['date'], errors='coerce')\n",
    "        min_date = dates.min()\n",
    "        temporal_features = (dates - min_date).dt.days.fillna(0).values\n",
    "    else:\n",
    "        temporal_features = np.zeros(len(df))\n",
    "    \n",
    "    # Combine features for x\n",
    "    combined_features = np.concatenate(features, axis=1) if features else np.zeros((len(df), 1))\n",
    "    return torch.tensor(combined_features, dtype=torch.float32), torch.tensor(temporal_features, dtype=torch.float32)\n",
    "\n",
    "def process_intervention_features(df: pd.DataFrame) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Process intervention features.\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Process numerical columns\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if not numerical_cols.empty:\n",
    "        numerical_features = df[numerical_cols].fillna(0).values\n",
    "        features.append(numerical_features)\n",
    "    \n",
    "    # Process categorical columns\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        if col != 'date':  # Handle dates separately\n",
    "            categories = pd.Categorical(df[col].fillna('UNKNOWN'))\n",
    "            features.append(categories.codes.reshape(-1, 1))\n",
    "    \n",
    "    # Process date column\n",
    "    if 'date' in df.columns:\n",
    "        dates = pd.to_datetime(df['date'], errors='coerce')\n",
    "        min_date = dates.min()\n",
    "        temporal_features = (dates - min_date).dt.days.fillna(0).values\n",
    "    else:\n",
    "        temporal_features = np.zeros(len(df))\n",
    "    \n",
    "    # Combine features for x\n",
    "    combined_features = np.concatenate(features, axis=1) if features else np.zeros((len(df), 1))\n",
    "    return torch.tensor(combined_features, dtype=torch.float32), torch.tensor(temporal_features, dtype=torch.float32)\n",
    "\n",
    "def process_facility_features(df: pd.DataFrame) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Process facility features.\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Process numerical columns\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if not numerical_cols.empty:\n",
    "        numerical_features = df[numerical_cols].fillna(0).values\n",
    "        features.append(numerical_features)\n",
    "    \n",
    "    # Process categorical columns\n",
    "    categorical_cols = ['name', 'city', 'country']\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            categories = pd.Categorical(df[col].fillna('UNKNOWN'))\n",
    "            features.append(categories.codes.reshape(-1, 1))\n",
    "    \n",
    "    # No temporal features for facilities\n",
    "    temporal_features = np.zeros(len(df))\n",
    "    \n",
    "    # Combine features for x\n",
    "    combined_features = np.concatenate(features, axis=1) if features else np.zeros((len(df), 1))\n",
    "    return torch.tensor(combined_features, dtype=torch.float32), torch.tensor(temporal_features, dtype=torch.float32)\n",
    "\n",
    "def create_graph_data(studies_df: pd.DataFrame, outcomes_df: pd.DataFrame,\n",
    "                             interventions_df: pd.DataFrame, facilities_df: pd.DataFrame,\n",
    "                             rdf_graph: Graph) -> HeteroData:\n",
    "    \"\"\"Create heterogeneous graph from dataframes and RDF graph.\"\"\"\n",
    "    data = HeteroData()\n",
    "    \n",
    "    # Create node features with improved processing\n",
    "    node_features = {}\n",
    "    time_dict = {}\n",
    "    \n",
    "    # Process each node type and store features and temporal values\n",
    "    for node_type, df, process_fn in [\n",
    "        ('study', studies_df, process_study_features),\n",
    "        ('outcome', outcomes_df, process_outcome_features),\n",
    "        ('intervention', interventions_df, process_intervention_features),\n",
    "        ('facility', facilities_df, process_facility_features)\n",
    "    ]:\n",
    "        features, temporal = process_fn(df)\n",
    "        node_features[node_type] = features\n",
    "        time_dict[node_type] = temporal\n",
    "    \n",
    "    # Add node features to graph\n",
    "    for node_type, features in node_features.items():\n",
    "        data[node_type].x = features\n",
    "    \n",
    "    # Add temporal features to graph\n",
    "    data.time_dict = time_dict\n",
    "    \n",
    "    # Add edges from RDF graph\n",
    "    add_edges_from_rdf(data, rdf_graph, node_features)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30347dd4-226b-4437-99f0-539d6e814cdc",
   "metadata": {},
   "source": [
    "### Training and Evaluation\n",
    "\n",
    "#### 1. Training Function\n",
    "\n",
    "Implements the training loop with validation and early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9f6f274d-06ea-426f-b9a3-9a647569cbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, val_data, test_data, num_epochs=100, lr=0.01):\n",
    "    \"\"\"Train the model using train/val/test splits.\"\"\"\n",
    "    print(\"\\nTraining model...\")\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    val_metrics = []\n",
    "    best_val_auc = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(train_data.x_dict, train_data.edge_index_dict)\n",
    "        loss = criterion(out, train_data['study'].y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(val_data.x_dict, val_data.edge_index_dict)\n",
    "            val_loss = criterion(val_out, val_data['study'].y)\n",
    "            \n",
    "            val_pred = torch.sigmoid(val_out).cpu().numpy()\n",
    "            val_true = val_data['study'].y.cpu().numpy()\n",
    "            \n",
    "            val_auc = roc_auc_score(val_true, val_pred)\n",
    "            val_ap = average_precision_score(val_true, val_pred)\n",
    "            \n",
    "            val_metrics.append({\n",
    "                'loss': val_loss.item(),\n",
    "                'auc': val_auc,\n",
    "                'ap': val_ap\n",
    "            })\n",
    "            \n",
    "            # Save best model\n",
    "            if val_auc > best_val_auc:\n",
    "                best_val_auc = val_auc\n",
    "                best_model = model.state_dict()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1:03d}:')\n",
    "            print(f'Train Loss: {loss.item():.4f}')\n",
    "            print(f'Val Loss: {val_loss.item():.4f}, Val AUC: {val_auc:.4f}, Val AP: {val_ap:.4f}')\n",
    "    \n",
    "    # Load best model and evaluate on test set\n",
    "    model.load_state_dict(best_model)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_out = model(test_data.x_dict, test_data.edge_index_dict)\n",
    "        test_loss = criterion(test_out, test_data['study'].y)\n",
    "        \n",
    "        test_pred = torch.sigmoid(test_out).cpu().numpy()\n",
    "        test_true = test_data['study'].y.cpu().numpy()\n",
    "        \n",
    "        test_auc = roc_auc_score(test_true, test_pred)\n",
    "        test_ap = average_precision_score(test_true, test_pred)\n",
    "    \n",
    "    print('\\nTest Results:')\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')\n",
    "    print(f'Test AUC: {test_auc:.4f}')\n",
    "    print(f'Test AP: {test_ap:.4f}')\n",
    "    \n",
    "    return train_losses, val_metrics, test_auc, test_ap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e0e452-4eb6-45df-a09b-17c637f99575",
   "metadata": {},
   "source": [
    "#### 2. Visualization Function\n",
    "\n",
    "Plots training and validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "323c51c5-50c2-452e-b077-6bfdaddec968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "\n",
    "# Function to compute classification metrics\n",
    "def compute_classification_metrics(y_true, y_pred, y_pred_proba=None):\n",
    "    \"\"\"\n",
    "    Compute classification metrics for binary/multi-class classification tasks.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "\n",
    "    # Accuracy\n",
    "    metrics[\"accuracy\"] = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Precision, Recall, F1-Score\n",
    "    metrics[\"precision\"] = precision_score(y_true, y_pred, average=\"weighted\")\n",
    "    metrics[\"recall\"] = recall_score(y_true, y_pred, average=\"weighted\")\n",
    "    metrics[\"f1_score\"] = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "    # ROC-AUC (only for binary classification)\n",
    "    if y_pred_proba is not None and len(np.unique(y_true)) == 2:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "        metrics[\"roc_auc\"] = auc(fpr, tpr)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    metrics[\"confusion_matrix\"] = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix\"):\n",
    "    \"\"\"\n",
    "    Plot a confusion matrix.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot ROC curve\n",
    "def plot_roc_curve(y_true, y_pred_proba, title=\"ROC Curve\"):\n",
    "    \"\"\"\n",
    "    Plot the ROC curve for binary classification.\n",
    "    \"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot Precision-Recall curve\n",
    "def plot_precision_recall_curve(y_true, y_pred_proba, title=\"Precision-Recall Curve\"):\n",
    "    \"\"\"\n",
    "    Plot the Precision-Recall curve for binary classification.\n",
    "    \"\"\"\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, color=\"blue\", lw=2, label=f\"PR curve (AUC = {pr_auc:.2f})\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot metrics\n",
    "def plot_metrics(train_losses, val_metrics, test_data, model):\n",
    "    \"\"\"Plot training metrics.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # Plot losses\n",
    "    axes[0].plot(train_losses, label='Train Loss')\n",
    "    axes[0].plot([m['loss'] for m in val_metrics], label='Val Loss')\n",
    "    axes[0].set_title('Loss')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Plot AUC\n",
    "    axes[1].plot([m['auc'] for m in val_metrics])\n",
    "    axes[1].set_title('Validation AUC')\n",
    "\n",
    "    # Plot AP\n",
    "    axes[2].plot([m['ap'] for m in val_metrics])\n",
    "    axes[2].set_title('Validation AP')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot ROC curve, Confusion Matrix, etc.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_out = model(test_data.x_dict, test_data.edge_index_dict)\n",
    "        test_pred = torch.sigmoid(test_out).cpu().numpy()\n",
    "        test_true = test_data['study'].y.cpu().numpy()\n",
    "\n",
    "        # Compute classification metrics\n",
    "        classification_metrics = compute_classification_metrics(test_true, (test_pred > 0.5).astype(int), test_pred)\n",
    "        print(\"\\nClassification Metrics:\")\n",
    "        for metric, value in classification_metrics.items():\n",
    "            if metric != \"confusion_matrix\":\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        plot_confusion_matrix(test_true, (test_pred > 0.5).astype(int), title=\"Confusion Matrix (Test Set)\")\n",
    "\n",
    "        # Plot ROC curve\n",
    "        plot_roc_curve(test_true, test_pred, title=\"ROC Curve (Test Set)\")\n",
    "\n",
    "        # Plot Precision-Recall curve\n",
    "        plot_precision_recall_curve(test_true, test_pred, title=\"Precision-Recall Curve (Test Set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4bef81-4e74-46b6-91ae-4889bde86dc9",
   "metadata": {},
   "source": [
    "### Main Execution\n",
    "\n",
    "Run the complete training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2f9ce994-5bb9-4cd7-880c-2b173ab59f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data(data_dir: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Load and preprocess CSV data.\"\"\"\n",
    "    # Load CSV files\n",
    "    studies_df = pd.read_csv(os.path.join(data_dir, 'studies.csv'))\n",
    "    outcomes_df = pd.read_csv(os.path.join(data_dir, 'outcomes.csv'))\n",
    "    interventions_df = pd.read_csv(os.path.join(data_dir, 'interventions.csv'))\n",
    "    facilities_df = pd.read_csv(os.path.join(data_dir, 'facilities.csv'))\n",
    "    \n",
    "    # Preprocess dates\n",
    "    for df in [studies_df, outcomes_df, interventions_df]:\n",
    "        date_columns = df.select_dtypes(include=['object']).columns\n",
    "        for col in date_columns:\n",
    "            if 'date' in col.lower():\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    return studies_df, outcomes_df, interventions_df, facilities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b0f48311-1305-4cbb-a36d-e1615bed2c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature dimensions:\n",
      "study: 5\n",
      "outcome: 2\n",
      "intervention: 1\n",
      "facility: 1\n",
      "\n",
      "Training model...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'time_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 68\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m test_auc, test_ap\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[68], line 52\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m model \u001b[38;5;241m=\u001b[39m HeteroGNN(\n\u001b[0;32m     45\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mmetadata(),\n\u001b[0;32m     46\u001b[0m     hidden_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[0;32m     47\u001b[0m     out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     48\u001b[0m     feature_dims\u001b[38;5;241m=\u001b[39mfeature_dims\n\u001b[0;32m     49\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m train_losses, val_metrics, test_auc, test_ap \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\n\u001b[0;32m     59\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Plot results\u001b[39;00m\n\u001b[0;32m     62\u001b[0m plot_metrics(train_losses, val_metrics, test_data, model)\n",
      "Cell \u001b[1;32mIn[62], line 18\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_data, val_data, test_data, num_epochs, lr)\u001b[0m\n\u001b[0;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 18\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstudy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39my)\n\u001b[0;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\semantic-gml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\semantic-gml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'time_dict'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Get feature dimensions for each node type\n",
    "    feature_dims = {\n",
    "        'study': data['study'].x.shape[1],\n",
    "        'outcome': data['outcome'].x.shape[1],\n",
    "        'intervention': data['intervention'].x.shape[1],\n",
    "        'facility': data['facility'].x.shape[1]\n",
    "    }\n",
    "    \n",
    "    print(\"\\nFeature dimensions:\")\n",
    "    for node_type, dim in feature_dims.items():\n",
    "        print(f\"{node_type}: {dim}\")\n",
    "    \n",
    "    # Split data into train/val/test\n",
    "    num_studies = len(studies_df)\n",
    "    train_idx, temp_idx = train_test_split(range(num_studies), test_size=0.3, random_state=42)\n",
    "    val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # Create train/val/test masks\n",
    "    train_mask = torch.zeros(num_studies, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(num_studies, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_studies, dtype=torch.bool)\n",
    "    \n",
    "    train_mask[train_idx] = True\n",
    "    val_mask[val_idx] = True\n",
    "    test_mask[test_idx] = True\n",
    "    \n",
    "    # Create train/val/test data\n",
    "    train_data = data.clone()\n",
    "    val_data = data.clone()\n",
    "    test_data = data.clone()\n",
    "    \n",
    "    # Add masks to data\n",
    "    train_data['study'].train_mask = train_mask\n",
    "    val_data['study'].val_mask = val_mask\n",
    "    test_data['study'].test_mask = test_mask\n",
    "    \n",
    "    # Move data to device\n",
    "    train_data = train_data.to(device)\n",
    "    val_data = val_data.to(device)\n",
    "    test_data = test_data.to(device)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = HeteroGNN(\n",
    "        metadata=data.metadata(),\n",
    "        hidden_channels=64,\n",
    "        out_channels=1,\n",
    "        feature_dims=feature_dims\n",
    "    ).to(device)\n",
    "    \n",
    "    # Train model\n",
    "    train_losses, val_metrics, test_auc, test_ap = train_model(\n",
    "        model=model,\n",
    "        train_data=train_data,\n",
    "        val_data=val_data,\n",
    "        test_data=test_data,\n",
    "        num_epochs=100,\n",
    "        lr=0.01\n",
    "    )\n",
    "    \n",
    "    # Plot results\n",
    "    plot_metrics(train_losses, val_metrics, test_data, model)\n",
    "    \n",
    "    return test_auc, test_ap\n",
    "        \n",
    "\n",
    "# Run the main function\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a07efdf-a43c-4f3e-94f7-7e4f32d6172f",
   "metadata": {},
   "source": [
    "## **Relevant Resources:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e2cd82-4482-4bbc-9d48-b657648184e0",
   "metadata": {},
   "source": [
    "1. **RelBench Dataset and Framework:**  \n",
    "   - Website: https://relbench.stanford.edu/  \n",
    "   - Documentation: https://relbench.stanford.edu/start/  \n",
    "   - GitHub Repository: https://github.com/snap-stanford/relbench  \n",
    "\n",
    "2. **R2RML (RDB to RDF Mapping Language):**  \n",
    "   - W3C Specification: https://www.w3.org/TR/r2rml/  \n",
    "   - Tools:  \n",
    "     - **RMLMapper:** https://github.com/RMLio/rmlmapper-java  \n",
    "     - **Ontop:** https://ontop-vkg.org/  \n",
    "     - **Apache Jena:** https://jena.apache.org/  \n",
    "\n",
    "3. **Graph Machine Learning Libraries:**  \n",
    "   - **PyTorch Geometric (PyG):** https://pytorch-geometric.readthedocs.io/  \n",
    "   - **DGL (Deep Graph Library):** https://www.dgl.ai/  \n",
    "   - **Graph Neural Networks (GNNs):** https://distill.pub/2021/gnn-intro/  \n",
    "\n",
    "4. **RDF to Graph Conversion Tools:**  \n",
    "   - **RDFLib:** https://rdflib.readthedocs.io/  \n",
    "   - **Apache Jena:** https://jena.apache.org/  \n",
    "\n",
    "5. **Evaluation Metrics for Machine Learning:**  \n",
    "   - **ROC-AUC:** https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html  \n",
    "   - **Accuracy:** https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html  \n",
    "   - **Precision, Recall, F1-Score:** https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "772baf6a-6a2a-477f-b3a1-92e3afd80e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Loading CSV data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anils\\AppData\\Local\\Temp\\ipykernel_16020\\3635617325.py:33: DtypeWarning: Columns (19,25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  studies_df = pd.read_csv('data/studies.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 249730 studies\n",
      "Loaded 411933 outcomes\n",
      "Loaded 3462 interventions\n",
      "Loaded 453233 facilities\n",
      "\n",
      "Loading RDF mappings...\n",
      "Warning: studies-rdf.ttl not found\n",
      "Warning: interventions-rdf.ttl not found\n",
      "Warning: facilities-rdf.ttl not found\n",
      "Warning: outcomes-rdf.ttl not found\n",
      "Warning: reported_event_totals-rdf.ttl not found\n",
      "Warning: drop_withdrawals-rdf.ttl not found\n",
      "Warning: sponsors_studies-rdf.ttl not found\n",
      "Warning: conditions_studies-rdf.ttl not found\n",
      "\n",
      "Creating heterogeneous graph data...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Tried to collect 'edge_index' but did not find any occurrences of it in any node and/or edge type\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 255\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m test_auc, test_ap\n\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[1;32m--> 255\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[80], line 243\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    230\u001b[0m model \u001b[38;5;241m=\u001b[39m HeteroGNN(\n\u001b[0;32m    231\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mmetadata(),\n\u001b[0;32m    232\u001b[0m     hidden_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    239\u001b[0m     }\n\u001b[0;32m    240\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m test_auc, test_ap \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\n\u001b[0;32m    250\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m test_auc, test_ap\n",
      "Cell \u001b[1;32mIn[80], line 164\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_data, val_data, test_data, num_epochs, lr)\u001b[0m\n\u001b[0;32m    162\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    163\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 164\u001b[0m out \u001b[38;5;241m=\u001b[39m model(train_data\u001b[38;5;241m.\u001b[39mx_dict, \u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index_dict\u001b[49m)\n\u001b[0;32m    165\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstudy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39my)\n\u001b[0;32m    166\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\semantic-gml\\lib\\site-packages\\torch_geometric\\data\\hetero_data.py:161\u001b[0m, in \u001b[0;36mHeteroData.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_global_store, key)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_dict$\u001b[39m\u001b[38;5;124m'\u001b[39m, key)):\n\u001b[1;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\semantic-gml\\lib\\site-packages\\torch_geometric\\data\\hetero_data.py:565\u001b[0m, in \u001b[0;36mHeteroData.collect\u001b[1;34m(self, key, allow_empty)\u001b[0m\n\u001b[0;32m    563\u001b[0m         mapping[subtype] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(store, key)\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_empty \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapping) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to collect \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but did not find any \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    566\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moccurrences of it in any node and/or edge type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mapping\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Tried to collect 'edge_index' but did not find any occurrences of it in any node and/or edge type\""
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdflib import Graph, URIRef, Literal, XSD\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import HeteroConv, SAGEConv, Linear\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from relbench.datasets  import get_dataset\n",
    "from relbench.tasks import get_task\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the dataset and task\n",
    "dataset = get_dataset(\"rel-trial\", download=True)\n",
    "task = get_task(\"rel-trial\", \"study-outcome\", download=True)\n",
    "train_table = task.get_table(\"train\")\n",
    "df = train_table.df\n",
    "\n",
    "# Load CSV data\n",
    "def load_csv_data():\n",
    "    \"\"\"Load CSV data files.\"\"\"\n",
    "    print(\"\\nLoading CSV data...\")\n",
    "    \n",
    "    studies_df = pd.read_csv('data/studies.csv')\n",
    "    outcomes_df = pd.read_csv('data/outcomes.csv')\n",
    "    interventions_df = pd.read_csv('data/interventions.csv')\n",
    "    facilities_df = pd.read_csv('data/facilities.csv')\n",
    "    \n",
    "    print(f\"Loaded {len(studies_df)} studies\")\n",
    "    print(f\"Loaded {len(outcomes_df)} outcomes\")\n",
    "    print(f\"Loaded {len(interventions_df)} interventions\")\n",
    "    print(f\"Loaded {len(facilities_df)} facilities\")\n",
    "    \n",
    "    return studies_df, outcomes_df, interventions_df, facilities_df\n",
    "\n",
    "# Load RDF mappings\n",
    "def load_rdf_mappings(output_folder):\n",
    "    \"\"\"Load RDF mappings from the output folder.\"\"\"\n",
    "    print(\"\\nLoading RDF mappings...\")\n",
    "    rdf_graph = Graph()\n",
    "    \n",
    "    rdf_files = [\n",
    "        'studies-rdf.ttl',\n",
    "        'interventions-rdf.ttl',\n",
    "        'facilities-rdf.ttl',\n",
    "        'outcomes-rdf.ttl',\n",
    "        'reported_event_totals-rdf.ttl',\n",
    "        'drop_withdrawals-rdf.ttl',\n",
    "        'sponsors_studies-rdf.ttl',\n",
    "        'conditions_studies-rdf.ttl'\n",
    "    ]\n",
    "    \n",
    "    for filename in rdf_files:\n",
    "        filepath = os.path.join(output_folder, filename)\n",
    "        if os.path.exists(filepath):\n",
    "            print(f\"Loading {filename}...\")\n",
    "            rdf_graph.parse(filepath, format=\"turtle\")\n",
    "            print(f\"Loaded {len(rdf_graph)} total triples\")\n",
    "        else:\n",
    "            print(f\"Warning: {filename} not found\")\n",
    "    \n",
    "    return rdf_graph\n",
    "\n",
    "# Create heterogeneous graph data\n",
    "def create_graph_data(studies_df, outcomes_df, interventions_df, facilities_df, rdf_graph):\n",
    "    \"\"\"Create heterogeneous graph data from CSV and RDF data.\"\"\"\n",
    "    print(\"\\nCreating heterogeneous graph data...\")\n",
    "    data = HeteroData()\n",
    "    \n",
    "    # Create node features\n",
    "    study_features = torch.tensor(studies_df.select_dtypes(include=[np.number]).fillna(0).values, dtype=torch.float32)\n",
    "    outcome_features = torch.tensor(outcomes_df.select_dtypes(include=[np.number]).fillna(0).values, dtype=torch.float32)\n",
    "    intervention_features = torch.tensor(interventions_df.select_dtypes(include=[np.number]).fillna(0).values, dtype=torch.float32)\n",
    "    facility_features = torch.tensor(facilities_df.select_dtypes(include=[np.number]).fillna(0).values, dtype=torch.float32)\n",
    "    \n",
    "    # Add node features to HeteroData\n",
    "    data['study'].x = study_features\n",
    "    data['outcome'].x = outcome_features\n",
    "    data['intervention'].x = intervention_features\n",
    "    data['facility'].x = facility_features\n",
    "    \n",
    "    # Create node ID mappings\n",
    "    node_id_maps = {\n",
    "        'study': {f'http://example.org/study/{i}': i for i in range(len(studies_df))},\n",
    "        'outcome': {f'http://example.org/outcome/{i}': i for i in range(len(outcomes_df))},\n",
    "        'intervention': {f'http://example.org/intervention/{i}': i for i in range(len(interventions_df))},\n",
    "        'facility': {f'http://example.org/facility/{i}': i for i in range(len(facilities_df))}\n",
    "    }\n",
    "    \n",
    "    # Extract edges from RDF graph\n",
    "    edges_by_type = {}\n",
    "    for s, p, o in rdf_graph:\n",
    "        if isinstance(s, URIRef) and isinstance(o, URIRef):\n",
    "            s_type = next((node_type for node_type, node_map in node_id_maps.items() if str(s) in node_map), None)\n",
    "            o_type = next((node_type for node_type, node_map in node_id_maps.items() if str(o) in node_map), None)\n",
    "            \n",
    "            if s_type is not None and o_type is not None:\n",
    "                edge_type = str(p).split('/')[-1].split('#')[-1]\n",
    "                edge_key = (s_type, edge_type, o_type)\n",
    "                if edge_key not in edges_by_type:\n",
    "                    edges_by_type[edge_key] = []\n",
    "                s_idx = node_id_maps[s_type][str(s)]\n",
    "                o_idx = node_id_maps[o_type][str(o)]\n",
    "                edges_by_type[edge_key].append((s_idx, o_idx))\n",
    "    \n",
    "    # Add edges to HeteroData\n",
    "    for (s_type, edge_type, o_type), edges in edges_by_type.items():\n",
    "        if len(edges) > 0:\n",
    "            edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "            data[s_type, edge_type, o_type].edge_index = edge_index\n",
    "            print(f\"Added {len(edges)} edges of type ({s_type}, {edge_type}, {o_type})\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Enhanced HeteroGNN model\n",
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, metadata, hidden_channels, out_channels, feature_dims):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.lins = torch.nn.ModuleDict()\n",
    "        \n",
    "        for node_type, dim in feature_dims.items():\n",
    "            self.lins[node_type] = Linear(dim, hidden_channels)\n",
    "        \n",
    "        for _ in range(2):\n",
    "            conv_dict = {}\n",
    "            for edge_type in metadata[1]:\n",
    "                src_type, _, dst_type = edge_type\n",
    "                conv_dict[edge_type] = SAGEConv(\n",
    "                    (hidden_channels, hidden_channels),\n",
    "                    hidden_channels\n",
    "                )\n",
    "            self.convs.append(HeteroConv(conv_dict, aggr='mean'))\n",
    "        \n",
    "        self.output = Linear(hidden_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = {node_type: self.lins[node_type](x) for node_type, x in x_dict.items()}\n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "            x_dict = {key: torch.relu(x) for key, x in x_dict.items()}\n",
    "        return self.output(x_dict['study'])\n",
    "\n",
    "# Training and evaluation\n",
    "def train_model(model, train_data, val_data, test_data, num_epochs=100, lr=0.01):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    best_val_auc = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(train_data.x_dict, train_data.edge_index_dict)\n",
    "        loss = criterion(out, train_data['study'].y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(val_data.x_dict, val_data.edge_index_dict)\n",
    "            val_pred = torch.sigmoid(val_out).cpu().numpy()\n",
    "            val_true = val_data['study'].y.cpu().numpy()\n",
    "            val_auc = roc_auc_score(val_true, val_pred)\n",
    "            \n",
    "            if val_auc > best_val_auc:\n",
    "                best_val_auc = val_auc\n",
    "                best_model = model.state_dict()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1:03d}: Train Loss: {loss.item():.4f}, Val AUC: {val_auc:.4f}')\n",
    "    \n",
    "    model.load_state_dict(best_model)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_out = model(test_data.x_dict, test_data.edge_index_dict)\n",
    "        test_pred = torch.sigmoid(test_out).cpu().numpy()\n",
    "        test_true = test_data['study'].y.cpu().numpy()\n",
    "        test_auc = roc_auc_score(test_true, test_pred)\n",
    "        test_ap = average_precision_score(test_true, test_pred)\n",
    "    \n",
    "    print(f'Test AUC: {test_auc:.4f}, Test AP: {test_ap:.4f}')\n",
    "    return test_auc, test_ap\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    studies_df, outcomes_df, interventions_df, facilities_df = load_csv_data()\n",
    "    rdf_graph = load_rdf_mappings('output')\n",
    "    data = create_graph_data(studies_df, outcomes_df, interventions_df, facilities_df, rdf_graph)\n",
    "    \n",
    "    # Add labels from the task\n",
    "    data['study'].y = torch.tensor(df['outcome'].values, dtype=torch.float32)\n",
    "    \n",
    "    # Split data into train/val/test\n",
    "    num_studies = len(studies_df)\n",
    "    train_idx, temp_idx = train_test_split(range(num_studies), test_size=0.3, random_state=42)\n",
    "    val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n",
    "    \n",
    "    train_mask = torch.zeros(num_studies, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(num_studies, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_studies, dtype=torch.bool)\n",
    "    \n",
    "    train_mask[train_idx] = True\n",
    "    val_mask[val_idx] = True\n",
    "    test_mask[test_idx] = True\n",
    "    \n",
    "    train_data = data.clone()\n",
    "    val_data = data.clone()\n",
    "    test_data = data.clone()\n",
    "    \n",
    "    train_data['study'].train_mask = train_mask\n",
    "    val_data['study'].val_mask = val_mask\n",
    "    test_data['study'].test_mask = test_mask\n",
    "    \n",
    "    train_data = train_data.to(device)\n",
    "    val_data = val_data.to(device)\n",
    "    test_data = test_data.to(device)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = HeteroGNN(\n",
    "        metadata=data.metadata(),\n",
    "        hidden_channels=64,\n",
    "        out_channels=1,\n",
    "        feature_dims={\n",
    "            'study': data['study'].x.shape[1],\n",
    "            'outcome': data['outcome'].x.shape[1],\n",
    "            'intervention': data['intervention'].x.shape[1],\n",
    "            'facility': data['facility'].x.shape[1]\n",
    "        }\n",
    "    ).to(device)\n",
    "    \n",
    "    # Train model\n",
    "    test_auc, test_ap = train_model(\n",
    "        model=model,\n",
    "        train_data=train_data,\n",
    "        val_data=val_data,\n",
    "        test_data=test_data,\n",
    "        num_epochs=100,\n",
    "        lr=0.01\n",
    "    )\n",
    "    \n",
    "    return test_auc, test_ap\n",
    "\n",
    "# Run the main function\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1979dcc8-fbed-468d-a9fd-e5351da665fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:semantic-gml]",
   "language": "python",
   "name": "conda-env-semantic-gml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
